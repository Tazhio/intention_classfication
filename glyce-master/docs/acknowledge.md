## Acknowledgement  

* Vanilla Glyce is developed based on the previous SOTA model. Therefore, we would like to thank those who release their code. 
    * [Chinese NER Using Lattice LSTM](https://arxiv.org/abs/1805.02023)
    * [Subword Encoding in Lattice LSTM for Chinese Word Segmentation](https://arxiv.org/abs/1810.12594)
    * [Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF](https://arxiv.org/abs/1704.01314)
    * [Bag-of-Words as Target for Neural Machine Translation](https://www.aclweb.org/anthology/P18-2053)
    * [Syntax for Semantic Role Labeling, To Be, Or Not To Be](https://www.aclweb.org/anthology/P18-1192)
    * [Bilateral Multi-Perspective Matching for Natural Language Sentences](https://arxiv.org/abs/1702.03814)


* Glyce-BERT is developed based on [PyTorch implementation by HuggingFace](https://github.com/huggingface/pytorch-pretrained-BERT). And pretrained BERT model is released by [Google's pre-trained models](https://github.com/google-research/bert). 

    * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 
    * [Pretrained PyTorch models for Google's BERT Released by HuggingFace](https://github.com/huggingface/pytorch-pretrained-BERT)